{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX TITAN X (0000:06:00.0)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "from loader import TieLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "cfg = EasyDict()\n",
    "cfg.SEQ_LENGTH = 250\n",
    "cfg.TILE_SIZE = 32\n",
    "cfg.OUT_SIZE = 1\n",
    "cfg.TRAIN = EasyDict()\n",
    "cfg.TRAIN.EPOCH = 0\n",
    "cfg.TRAIN.EPOCH_SIZE = 1000\n",
    "cfg.TRAIN.EPOCH_NUM = 20\n",
    "cfg.gm_num = 4\n",
    "cfg.ndim = 12\n",
    "cfg.NAME = 'unet_true_grad%i'%(cfg.ndim)#'conv_net_no_bn_ndim12010'#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader = TieLoader('data/ties32',\n",
    "                         0.2,0.3,t_size=32,mask_size=cfg.OUT_SIZE,sample_size=cfg.TILE_SIZE,cache_samples=True)\n",
    "test_loader = TieLoader('data/test_ties32',\n",
    "                        -0.1,0.45,t_size=32,mask_size=cfg.OUT_SIZE,sample_size=cfg.TILE_SIZE,cache_samples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from dataset_tools import draw\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# for x,y in data_generator(test_loader,epoch_size=1,shuffle=False):\n",
    "#     print x.shape\n",
    "#     print np.ones_like(y)[y<0.1].sum(),np.ones_like(y)[y>0.9].sum()\n",
    "#     x = np.transpose(x,(0,2,3,1)).astype(np.uint8)\n",
    "#     y = (y*255.).astype(np.uint8)\n",
    "#     _y = np.zeros(x.shape[:3],dtype=np.uint8)\n",
    "    \n",
    "#     _y[:] = y[:,0,0][:,None,None]\n",
    "#     draw(x,_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained UNet_params_pretrained\n",
      "\n",
      "In -->      Layer    --> Out     Description                 \n",
      "-------     -----    -------     -----------                 \n",
      "[]          0        [1]         input(None, 3, 32, 32)      \n",
      "[0]         1        [2]         None(None, 16, 32, 32)      \n",
      "[1]         2        [3]         None(None, 16, 32, 32)      \n",
      "[2]         3        [4]         contr_1_1(None, 16, 32, 32) \n",
      "[3]         4        [5]         None(None, 16, 32, 32)      \n",
      "[4]         5        [6]         None(None, 16, 32, 32)      \n",
      "[5]         6        [7, 68]     contr_1_2(None, 16, 32, 32) \n",
      "[6]         7        [8]         pool1(None, 16, 16, 16)     \n",
      "[7]         8        [9]         None(None, 32, 16, 16)      \n",
      "[8]         9        [10]        None(None, 32, 16, 16)      \n",
      "[9]         10       [11]        contr_2_1(None, 32, 16, 16) \n",
      "[10]        11       [12]        None(None, 32, 16, 16)      \n",
      "[11]        12       [13]        None(None, 32, 16, 16)      \n",
      "[12]        13       [14, 58]    contr_2_2(None, 32, 16, 16) \n",
      "[13]        14       [15]        pool2(None, 32, 8, 8)       \n",
      "[14]        15       [16]        None(None, 64, 8, 8)        \n",
      "[15]        16       [17]        None(None, 64, 8, 8)        \n",
      "[16]        17       [18]        contr_3_1(None, 64, 8, 8)   \n",
      "[17]        18       [19]        None(None, 64, 8, 8)        \n",
      "[18]        19       [20]        None(None, 64, 8, 8)        \n",
      "[19]        20       [21, 48]    contr_3_2(None, 64, 8, 8)   \n",
      "[20]        21       [22]        pool3(None, 64, 4, 4)       \n",
      "[21]        22       [23]        None(None, 128, 4, 4)       \n",
      "[22]        23       [24]        None(None, 128, 4, 4)       \n",
      "[23]        24       [25]        contr_4_1(None, 128, 4, 4)  \n",
      "[24]        25       [26]        None(None, 128, 4, 4)       \n",
      "[25]        26       [27]        None(None, 128, 4, 4)       \n",
      "[26]        27       [28, 38]    contr_4_2(None, 128, 4, 4)  \n",
      "[27]        28       [29]        pool4(None, 128, 2, 2)      \n",
      "[28]        29       [30]        None(None, 256, 2, 2)       \n",
      "[29]        30       [31]        None(None, 256, 2, 2)       \n",
      "[30]        31       [32]        encode_1(None, 256, 2, 2)   \n",
      "[31]        32       [33]        None(None, 256, 2, 2)       \n",
      "[32]        33       [34]        None(None, 256, 2, 2)       \n",
      "[33]        34       [35]        encode_2(None, 256, 2, 2)   \n",
      "[34]        35       [36]        None(None, 256, 4, 4)       \n",
      "[35]        36       [37]        None(None, 256, 4, 4)       \n",
      "[36]        37       [38]        upscale1(None, 256, 4, 4)   \n",
      "[37, 27]    38       [39]        concat1(None, 384, 4, 4)    \n",
      "[38]        39       [40]        None(None, 128, 4, 4)       \n",
      "[39]        40       [41]        None(None, 128, 4, 4)       \n",
      "[40]        41       [42]        expand_1_1(None, 128, 4, 4) \n",
      "[41]        42       [43]        None(None, 128, 4, 4)       \n",
      "[42]        43       [44]        None(None, 128, 4, 4)       \n",
      "[43]        44       [45]        expand_1_2(None, 128, 4, 4) \n",
      "[44]        45       [46]        None(None, 128, 8, 8)       \n",
      "[45]        46       [47]        None(None, 128, 8, 8)       \n",
      "[46]        47       [48]        upscale2(None, 128, 8, 8)   \n",
      "[47, 20]    48       [49]        concat2(None, 192, 8, 8)    \n",
      "[48]        49       [50]        None(None, 64, 8, 8)        \n",
      "[49]        50       [51]        None(None, 64, 8, 8)        \n",
      "[50]        51       [52]        expand_2_1(None, 64, 8, 8)  \n",
      "[51]        52       [53]        None(None, 64, 8, 8)        \n",
      "[52]        53       [54]        None(None, 64, 8, 8)        \n",
      "[53]        54       [55]        expand_2_2(None, 64, 8, 8)  \n",
      "[54]        55       [56]        None(None, 64, 16, 16)      \n",
      "[55]        56       [57]        None(None, 64, 16, 16)      \n",
      "[56]        57       [58]        upscale3(None, 64, 16, 16)  \n",
      "[57, 13]    58       [59]        concat3(None, 96, 16, 16)   \n",
      "[58]        59       [60]        None(None, 32, 16, 16)      \n",
      "[59]        60       [61]        None(None, 32, 16, 16)      \n",
      "[60]        61       [62]        expand_3_1(None, 32, 16, 16)\n",
      "[61]        62       [63]        None(None, 32, 16, 16)      \n",
      "[62]        63       [64]        None(None, 32, 16, 16)      \n",
      "[63]        64       [65]        expand_3_2(None, 32, 16, 16)\n",
      "[64]        65       [66]        None(None, 32, 32, 32)      \n",
      "[65]        66       [67]        None(None, 32, 32, 32)      \n",
      "[66]        67       [68]        upscale4(None, 32, 32, 32)  \n",
      "[67, 6]     68       [69]        concat4(None, 48, 32, 32)   \n",
      "[68]        69       [70]        None(None, 16, 32, 32)      \n",
      "[69]        70       [71]        None(None, 16, 32, 32)      \n",
      "[70]        71       [72]        expand_4_1(None, 16, 32, 32)\n",
      "[71]        72       [73]        None(None, 16, 32, 32)      \n",
      "[72]        73       [74]        None(None, 16, 32, 32)      \n",
      "[73]        74       [75]        expand_4_2(None, 16, 32, 32)\n",
      "[74]        75       [76]        None(None, 11, 32, 32)      \n",
      "[75]        76       [77]        transpose(None, 32, 32, 11) \n",
      "[76]        77       []          l2norm(None, 32, 32, 12)    \n"
     ]
    }
   ],
   "source": [
    "from lasagne import layers as L\n",
    "from lasagne.nonlinearities import rectify,softmax\n",
    "from utils import NormedDense,L2NormLayer\n",
    "from utils import get_network_str,load_weights\n",
    "import lasagne\n",
    "import denseNet\n",
    "import uNet\n",
    "from networks import baseline_norm,conv4_net,conv4_net_dense\n",
    "\n",
    "\n",
    "def FCN(data,ndim,verbose=True,model_name='',input_shape = (None,3,None,None),pad='same',logger=None):\n",
    "    datal = res = L.InputLayer(input_shape\n",
    "                           ,data\n",
    "                           ,name='data')\n",
    "    res = uNet.build(datal)\n",
    "    res = L.Conv2DLayer(res, ndim-1, 1, nonlinearity=None)\n",
    "    res.W.name = 'outW'\n",
    "    res.b.name = 'outb'\n",
    "    res = L.DimshuffleLayer(res,(0,2,3,1),name='transpose')\n",
    "    res = L2NormLayer(res,1e-8,name='l2norm')\n",
    "    if(model_name!=''):\n",
    "        print 'load model '+'models/'+model_name\n",
    "        load_weights(res,'models/'+model_name)\n",
    "    print get_network_str(res,incomings=True,outgoings=True)\n",
    "    if(not (logger is None)):        \n",
    "        logger.write(get_network_str(res,incomings=True,outgoings=True)+'\\n')\n",
    "        logger.flush()\n",
    "    return res\n",
    "        \n",
    "\n",
    "data = T.tensor4(name='data')\n",
    "label = T.tensor3(name='label')\n",
    "net = FCN(data,ndim=cfg.ndim,model_name='%s%03d'%(cfg.NAME,cfg.TRAIN.EPOCH) if cfg.TRAIN.EPOCH > 0 else '',\n",
    "          input_shape = (None,3,cfg.TILE_SIZE,cfg.TILE_SIZE),\n",
    "          pad = 'same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode=self.mode,\n",
      "/usr/local/lib/python2.7/dist-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode=self.mode,\n",
      "/usr/local/lib/python2.7/dist-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode=self.mode,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_fn compiled\n",
      "test_fn compiled\n"
     ]
    }
   ],
   "source": [
    "from gmm_op import get_gmm,calc_log_prob_gmm\n",
    "from theano_utils import split,histogram_loss,split_tr_p_n\n",
    "\n",
    "def make_classifier(X,label,non_learn_params):\n",
    "    X = X.reshape((-1,X.shape[-1]))\n",
    "    x_tr,x_p,x_n = split_tr_p_n(X,label.flatten())\n",
    "    m,c,w = get_gmm(x_tr,cfg.gm_num,cfg.ndim,use_approx_grad=False)\n",
    "    p_n = calc_log_prob_gmm(x_n,m,c,w)\n",
    "    p_p = calc_log_prob_gmm(x_p,m,c,w)\n",
    "    loss = histogram_loss(p_n,p_p,\n",
    "                          non_learn_params['min_cov'],\n",
    "                          100,\n",
    "                          non_learn_params['width'])[0]\n",
    "    prediction = T.nnet.sigmoid(T.concatenate([p_p,p_n],axis=0))\n",
    "    Y = T.concatenate([T.ones_like(p_p),T.zeros_like(p_n)],axis=0)\n",
    "    return loss,X,Y,prediction,m,c,w,p_p,p_n\n",
    "\n",
    "def make_train(net,data,label,non_learn_params):\n",
    "    sym = L.get_output(net ,deterministic=False)\n",
    "    s = int((L.get_output_shape(net)[1]-1)/2)\n",
    "    sym = sym[:,s:s+1,s:s+1,:]\n",
    "    loss,X,Y,prediction,m,c,w,p_p,p_n = make_classifier(sym,label,non_learn_params)\n",
    "    params = L.get_all_params(net,trainable=True)\n",
    "    updates = lasagne.updates.adam(loss,params,non_learn_params['lr'])\n",
    "    return theano.function([data, label], [loss,X,Y,prediction,m,c,w],\\\n",
    "                               allow_input_downcast=True, updates=updates)\n",
    "\n",
    "\n",
    "def make_test(net,data,label,non_learn_params):\n",
    "    sym = L.get_output(net ,deterministic=True)\n",
    "    s = int((L.get_output_shape(net)[1]-1)/2)\n",
    "    sym = sym[:,s:s+1,s:s+1]\n",
    "    loss,X,Y,prediction,m,c,w,p_p,p_n = make_classifier(sym,label,non_learn_params)\n",
    "    return theano.function([data, label], [loss,X,Y,prediction,m,c,w],\\\n",
    "                               allow_input_downcast=True)\n",
    "        \n",
    "\n",
    "non_learn_params={'min_cov' : theano.shared(1e-3),\n",
    "                  'lr' : theano.shared(np.array(1e-2, dtype=theano.config.floatX)),\n",
    "                  'width': theano.shared(4.)}\n",
    "\n",
    "train_fn = make_train(net,data,label,non_learn_params)\n",
    "print 'train_fn compiled'\n",
    "test_fn = make_test(net,data,label,non_learn_params)\n",
    "print 'test_fn compiled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint models/unet_true_grad12000.npz\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import datetime\n",
    "from utils import get_pp_pn\n",
    "import os\n",
    "from train_tools import fit\n",
    "from test_tools import test_network\n",
    "\n",
    "metrix = { 'aps' : average_precision_score,\n",
    "           'pp'  : lambda l,pred : np.ones(len(l))[(pred < 0.5)&(l<0.1)].sum()/np.ones(len(l))[l<0.1].sum(),\n",
    "           'pn'  : lambda l,pred : np.ones(len(l))[(pred >= .5)&(l>0.9)].sum()/np.ones(len(l))[l>0.9].sum(),\n",
    "           'int_pp_pn' : lambda l,pred : get_pp_pn(l,pred)}\n",
    "\n",
    "def update_params(epoch,params):\n",
    "    if(epoch == 0):\n",
    "        params['min_cov'].set_value(1e-4)\n",
    "        params['lr'].set_value(5e-2)\n",
    "    if(epoch == 4):\n",
    "        params['min_cov'].set_value(1e-8)\n",
    "        params['lr'].set_value(5e-3)\n",
    "    if(epoch == 10):\n",
    "        params['lr'].set_value(1e-3)\n",
    "        \n",
    "fout = open(\"out.txt\",'a')\n",
    "fout.write('################### train network '+cfg.NAME+ ' ' + str(datetime.datetime.now())+'################\\n')    \n",
    "fout.write(str(cfg))\n",
    "fit(cfg.NAME,net,train_fn,test_fn,train_loader,test_loader,non_learn_params,\n",
    "    update_params=update_params,\n",
    "    metrix = metrix,\n",
    "    logger=fout,\n",
    "    train_esize = 1500,\n",
    "    test_esize = 750,\n",
    "    epochs=cfg.TRAIN.EPOCH_NUM-cfg.TRAIN.EPOCH)\n",
    "\n",
    "test_network(cfg.NAME,FCN,cfg.ndim,cfg.TRAIN.EPOCH_NUM-1,cfg.gm_num)\n",
    "fout.write('################### done #######################\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_learn_params['lr'].set_value(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,batch in enumerate(data_generator(test_loader,epoch_size=100,shuffle=False)):\n",
    "    res = test_fn(*batch)\n",
    "    if(i == 0):\n",
    "        p =res[-1]\n",
    "        n = res[-2]\n",
    "    p = np.concatenate((p,res[-1]))\n",
    "    n = np.concatenate((n,res[-2]))\n",
    "plt.hist(np.clip(res[-1],-10,100),100,normed=True)\n",
    "plt.hist(np.clip(res[-2],-10,100),100,normed=True)\n",
    "plt.show()  \n",
    "plt.hist(np.clip(p,-20,100),100,normed=True)\n",
    "plt.hist(np.clip(n,-20,100),100,normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "#from gmm_bg_substructor import BgSubstructor\n",
    "from sklearn import mixture\n",
    "from sklearn.metrics import average_precision_score\n",
    "from dataset_tools import *\n",
    "from gmm_op import get_gmm,calc_log_prob_gmm\n",
    "import theano.tensor as T\n",
    "from lasagne import layers as L\n",
    "import theano\n",
    "\n",
    "def soft_predict_sym(features,means,covars,weights):\n",
    "    return 1.-T.nnet.sigmoid(calc_log_prob_gmm(features,means,covars,weights))\n",
    "\n",
    "\n",
    "def make_features(feature_fn,imgs):\n",
    "    data = None\n",
    "    for i in range(len(imgs)):\n",
    "        tmp = feature_fn(np.transpose(imgs[i:i+1],(0,3,1,2)).astype(np.float32))[0]\n",
    "        if(data is None):\n",
    "            data = np.empty((len(imgs),)+tmp.shape,dtype=np.float32)\n",
    "        data[i] = tmp\n",
    "    return data\n",
    "\n",
    "def make_gmms(shape,gm_num):\n",
    "    gmms = []\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            gmms.append(mixture.GaussianMixture(covariance_type='diag',\n",
    "                               n_components=gm_num,\n",
    "                               max_iter=1000,\n",
    "                               warm_start=True))\n",
    "    return gmms\n",
    "\n",
    "    \n",
    "def fit_gmms(features,gmms,masks = None,out_masks=None):\n",
    "    print features.shape\n",
    "    for i in range(features.shape[1]):\n",
    "        for j in range(features.shape[2]):\n",
    "            f = features[:,i,j]\n",
    "            gmm = gmms[i*features.shape[2]+j]\n",
    "            if not (masks is None):\n",
    "                if(len(f[masks[:,i,j] < 30]) > gmm.n_components*3):\n",
    "                    f = f[masks[:,i,j] < 30]\n",
    "            \n",
    "            if not (masks is None):\n",
    "                m = masks[:,i,j] \n",
    "                if(m[(m > 30) & (m< 240)].size > 0.9*m.size):\n",
    "                    out_masks[:,i,j] = 128\n",
    "                    continue\n",
    "                else:\n",
    "                    gmm.fit(f)\n",
    "            else:\n",
    "                gmm.fit(f)\n",
    "\n",
    "def predict_pixelwise(features,gmms,predict_fn,masks):\n",
    "    res = np.zeros_like(features[:,:,:,0])\n",
    "    for i in range(features.shape[1]):\n",
    "        for j in range(features.shape[2]):\n",
    "            m = masks[:,i,j] \n",
    "            if(m[(m > 30) & (m< 240)].size > 0.9*m.size):\n",
    "                continue\n",
    "            gmm = gmms[i*features.shape[2]+j]\n",
    "            res[:,i,j] = predict_fn(features[:,i,j,:],gmm.means_,gmm.covariances_,gmm.weights_)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def calc_aps(pred,masks):\n",
    "    masks = masks.flatten()\n",
    "    pred = pred.flatten()\n",
    "    m = np.zeros_like(masks)\n",
    "    m[masks > 240] = 1.\n",
    "    m = m[(masks<30) | (masks > 240)]\n",
    "    pred = pred[(masks<30) | (masks > 240)]\n",
    "    return average_precision_score(m,1.-pred)\n",
    "\n",
    "def make_test_as_train(feature_fn,predict_fn,\n",
    "                       gm_num,\n",
    "                      out_dir,\n",
    "                      dataset='dataset',\n",
    "                      max_frames=400,\n",
    "                      im_size = (320,240),\n",
    "                      train_size = 100):\n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        pass\n",
    "    aps_log = open(out_dir+'/aps.txt','w')\n",
    "    for in_dir,out_dir in iterate_folders(dataset,out_dir):\n",
    "        print 'run in ',in_dir\n",
    "        for i,(imgs,masks) in enumerate(iterate_bathced(in_dir,max_frames,im_size)):\n",
    "            if((masks[(masks>30) & (masks < 240)].size > 0.9*masks.size) or\n",
    "               (masks[(masks>240)].size < 10)):\n",
    "                print 'skip' \n",
    "                continue\n",
    "            print in_dir,'generate_features,',\n",
    "            data = make_features(feature_fn,imgs)\n",
    "            gmms = make_gmms(imgs.shape[1:-1],gm_num)\n",
    "            print 'fit gmms,',\n",
    "            fit_gmms(data[:train_size],gmms,masks[:train_size],masks[train_size:])\n",
    "            print 'predict,',\n",
    "            prediction = predict_pixelwise(data[train_size:],gmms,predict_fn,masks[train_size:])\n",
    "            aps = calc_aps(prediction,masks[train_size:])\n",
    "            print in_dir,'aps = ',aps\n",
    "            aps_log.write(in_dir+' : '+str(aps)+'\\n')\n",
    "            aps_log.flush()\n",
    "            print 'save'\n",
    "            imgs = imgs[train_size:]\n",
    "            masks = masks[train_size:]      \n",
    "            prediction = (prediction*255).astype(np.uint8)\n",
    "            for i in range(len(imgs)):\n",
    "                cv2.imwrite(out_dir+'/'+str(i)+'.png',prediction[i])\n",
    "                cv2.imwrite(out_dir+'/'+str(i)+'_true.png',masks[i])\n",
    "                cv2.imwrite(out_dir+'/'+str(i)+'_input.jpg',imgs[i])\n",
    "            break\n",
    "        print ''\n",
    "    print 'test complete'\n",
    "\n",
    "def test_network(name,network,ndim,epoch,gm_num):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained UNet_params_pretrained\n",
      "load model models/unet12019\n",
      "\n",
      "In -->      Layer    --> Out     Description                  \n",
      "-------     -----    -------     -----------                  \n",
      "[]          0        [1]         input(500, 3, 240, 320)      \n",
      "[0]         1        [2]         None(500, 16, 240, 320)      \n",
      "[1]         2        [3]         None(500, 16, 240, 320)      \n",
      "[2]         3        [4]         contr_1_1(500, 16, 240, 320) \n",
      "[3]         4        [5]         None(500, 16, 240, 320)      \n",
      "[4]         5        [6]         None(500, 16, 240, 320)      \n",
      "[5]         6        [7, 68]     contr_1_2(500, 16, 240, 320) \n",
      "[6]         7        [8]         pool1(500, 16, 120, 160)     \n",
      "[7]         8        [9]         None(500, 32, 120, 160)      \n",
      "[8]         9        [10]        None(500, 32, 120, 160)      \n",
      "[9]         10       [11]        contr_2_1(500, 32, 120, 160) \n",
      "[10]        11       [12]        None(500, 32, 120, 160)      \n",
      "[11]        12       [13]        None(500, 32, 120, 160)      \n",
      "[12]        13       [14, 58]    contr_2_2(500, 32, 120, 160) \n",
      "[13]        14       [15]        pool2(500, 32, 60, 80)       \n",
      "[14]        15       [16]        None(500, 64, 60, 80)        \n",
      "[15]        16       [17]        None(500, 64, 60, 80)        \n",
      "[16]        17       [18]        contr_3_1(500, 64, 60, 80)   \n",
      "[17]        18       [19]        None(500, 64, 60, 80)        \n",
      "[18]        19       [20]        None(500, 64, 60, 80)        \n",
      "[19]        20       [21, 48]    contr_3_2(500, 64, 60, 80)   \n",
      "[20]        21       [22]        pool3(500, 64, 30, 40)       \n",
      "[21]        22       [23]        None(500, 128, 30, 40)       \n",
      "[22]        23       [24]        None(500, 128, 30, 40)       \n",
      "[23]        24       [25]        contr_4_1(500, 128, 30, 40)  \n",
      "[24]        25       [26]        None(500, 128, 30, 40)       \n",
      "[25]        26       [27]        None(500, 128, 30, 40)       \n",
      "[26]        27       [28, 38]    contr_4_2(500, 128, 30, 40)  \n",
      "[27]        28       [29]        pool4(500, 128, 15, 20)      \n",
      "[28]        29       [30]        None(500, 256, 15, 20)       \n",
      "[29]        30       [31]        None(500, 256, 15, 20)       \n",
      "[30]        31       [32]        encode_1(500, 256, 15, 20)   \n",
      "[31]        32       [33]        None(500, 256, 15, 20)       \n",
      "[32]        33       [34]        None(500, 256, 15, 20)       \n",
      "[33]        34       [35]        encode_2(500, 256, 15, 20)   \n",
      "[34]        35       [36]        None(500, 256, 30, 40)       \n",
      "[35]        36       [37]        None(500, 256, 30, 40)       \n",
      "[36]        37       [38]        upscale1(500, 256, 30, 40)   \n",
      "[37, 27]    38       [39]        concat1(500, 384, 30, 40)    \n",
      "[38]        39       [40]        None(500, 128, 30, 40)       \n",
      "[39]        40       [41]        None(500, 128, 30, 40)       \n",
      "[40]        41       [42]        expand_1_1(500, 128, 30, 40) \n",
      "[41]        42       [43]        None(500, 128, 30, 40)       \n",
      "[42]        43       [44]        None(500, 128, 30, 40)       \n",
      "[43]        44       [45]        expand_1_2(500, 128, 30, 40) \n",
      "[44]        45       [46]        None(500, 128, 60, 80)       \n",
      "[45]        46       [47]        None(500, 128, 60, 80)       \n",
      "[46]        47       [48]        upscale2(500, 128, 60, 80)   \n",
      "[47, 20]    48       [49]        concat2(500, 192, 60, 80)    \n",
      "[48]        49       [50]        None(500, 64, 60, 80)        \n",
      "[49]        50       [51]        None(500, 64, 60, 80)        \n",
      "[50]        51       [52]        expand_2_1(500, 64, 60, 80)  \n",
      "[51]        52       [53]        None(500, 64, 60, 80)        \n",
      "[52]        53       [54]        None(500, 64, 60, 80)        \n",
      "[53]        54       [55]        expand_2_2(500, 64, 60, 80)  \n",
      "[54]        55       [56]        None(500, 64, 120, 160)      \n",
      "[55]        56       [57]        None(500, 64, 120, 160)      \n",
      "[56]        57       [58]        upscale3(500, 64, 120, 160)  \n",
      "[57, 13]    58       [59]        concat3(500, 96, 120, 160)   \n",
      "[58]        59       [60]        None(500, 32, 120, 160)      \n",
      "[59]        60       [61]        None(500, 32, 120, 160)      \n",
      "[60]        61       [62]        expand_3_1(500, 32, 120, 160)\n",
      "[61]        62       [63]        None(500, 32, 120, 160)      \n",
      "[62]        63       [64]        None(500, 32, 120, 160)      \n",
      "[63]        64       [65]        expand_3_2(500, 32, 120, 160)\n",
      "[64]        65       [66]        None(500, 32, 240, 320)      \n",
      "[65]        66       [67]        None(500, 32, 240, 320)      \n",
      "[66]        67       [68]        upscale4(500, 32, 240, 320)  \n",
      "[67, 6]     68       [69]        concat4(500, 48, 240, 320)   \n",
      "[68]        69       [70]        None(500, 16, 240, 320)      \n",
      "[69]        70       [71]        None(500, 16, 240, 320)      \n",
      "[70]        71       [72]        expand_4_1(500, 16, 240, 320)\n",
      "[71]        72       [73]        None(500, 16, 240, 320)      \n",
      "[72]        73       [74]        None(500, 16, 240, 320)      \n",
      "[73]        74       [75]        expand_4_2(500, 16, 240, 320)\n",
      "[74]        75       [76]        None(500, 11, 240, 320)      \n",
      "[75]        76       [77]        transpose(500, 240, 320, 11) \n",
      "[76]        77       []          l2norm(500, 240, 320, 12)    \n",
      "run in  data/test/turbulence/turbulence0\n",
      "data/test/turbulence/turbulence0 generate_features, fit gmms, (100, 240, 320, 12)\n",
      "predict, data/test/turbulence/turbulence0 aps =  9.902255715190797e-05\n",
      "save\n",
      "\n",
      "run in  data/test/thermal/corridor\n",
      "data/test/thermal/corridor generate_features, fit gmms, (100, 240, 320, 12)\n"
     ]
    }
   ],
   "source": [
    "test_network(cfg.NAME,FCN,cfg.ndim,cfg.TRAIN.EPOCH_NUM-1,cfg.gm_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
